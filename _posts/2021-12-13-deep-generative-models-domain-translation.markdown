---
layout: single
title:  "A Dissertation on Deep Generative Models for Domain Translation [Spanish]"
excerpt: "An overview of deep generative model families for cross-modal generation, particularly between textual, audio, and still image or video."
date: 2021-12-13 09:00:00
classes: wide
header:
  image: /assets/domain_translation/domain_translation_header.jpg
  teaser: /assets/domain_translation/domain_translation_header.jpg
tags: 
    - domain translation
    - cross-modal learning
    - deep generative models
    - generative adversarial networks
    - variational autoencoders
    - transformers
---

<!-- In this video (in Spanish) you will learn about the main families of deep generative models, namely auto-regressive models, such as PixelCNN or WaveNet, Variational Auto-Encoders (VAE), conditional Generative Adversarial Networks (cGANs) and Transformers, and how they can be applied for domain translation, aka cross-modal generation between disparete domains such as text-to-audio or audio-to-image. -->

Over the last decade Deep Learning has revolutionized almost any ICT area, permeating so fast and deeply in companies and social spheres that has enabled new capabilities never seen before. Some of them already so quotidian as the controversial, fake photorealistic portraits, also known as deepfakes. 

To this effect, Deep Generative Models have experimented an intense and steady improvement in their capabilities, including various paradigm changes. Some of their variants enable impressive techniques for Multimodal Domain Translation (MDT), by which for instance an image can be generated from a simple textual caption, or a soundless video excerpt populated with a synthetic and plausible audio track. State of the art Conditional Generative Adversarial Networks (cGANs) possess some benefits and demonstrate with outstanding results over Variational Autoencoders (VAEs), in a competitive race where Transformers have recently irrupted with high levels of expectation.

In this video we walk along various MDT creative applications and examples of image-to-image, text-to-sound, or video-to-sound translation. Concurrently, multiple techniques for improving and stabilizing cGANs training will be threshed, such as conditioning augmentation, spectral normalization, adaptive data augmentation, gradient penalty, self-attention, class conditional normalization, or auxiliary and conditional projection classifiers. A concluding visit to attentional architectures, shaping the core of the Transformers’ paradigm, will show their unique capabilities as translators —beyond Natural Language Processing such as GPT-3— and as generators in general.

For those non-spanish speakers, the slides in english can be directly downloaded [here](https://www.dropbox.com/s/8o2uqkes90j32xi/Masterclass-DomainTranslation-Eurecat-RRT.pdf?dl=0).

Thanks for watching!

<iframe width="560" height="315" src="https://www.youtube.com/embed/qZkXPucK0WU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


